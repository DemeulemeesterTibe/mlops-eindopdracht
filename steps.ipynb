{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tibed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tibed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "def lemmatization(text):\n",
    "    lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    text=[lemmatizer.lemmatize(y) for y in text]\n",
    "    \n",
    "    return \" \" .join(text)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "\n",
    "    Text=[i for i in str(text).split() if i not in stop_words]\n",
    "    return \" \".join(Text)\n",
    "\n",
    "def Removing_numbers(text):\n",
    "    text=''.join([i for i in text if not i.isdigit()])\n",
    "    return text\n",
    "\n",
    "def lower_case(text):\n",
    "    \n",
    "    text = text.split()\n",
    "\n",
    "    text=[y.lower() for y in text]\n",
    "    \n",
    "    return \" \" .join(text)\n",
    "\n",
    "def Removing_punctuations(text):\n",
    "    ## Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
    "    text = text.replace('؛',\"\", )\n",
    "    \n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text =  \" \".join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "def Removing_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_small_sentences(df):\n",
    "    for i in range(len(df)):\n",
    "        if len(df.text.iloc[i].split()) < 3:\n",
    "            df.text.iloc[i] = np.nan\n",
    "            \n",
    "def normalize_text(df):\n",
    "    df.content=df.content.apply(lambda text : lower_case(text))\n",
    "    df.content=df.content.apply(lambda text : remove_stop_words(text))\n",
    "    df.content=df.content.apply(lambda text : Removing_numbers(text))\n",
    "    df.content=df.content.apply(lambda text : Removing_punctuations(text))\n",
    "    df.content=df.content.apply(lambda text : Removing_urls(text))\n",
    "    df.content=df.content.apply(lambda text : lemmatization(text))\n",
    "    return df\n",
    "\n",
    "def normalized_sentence(sentence):\n",
    "    sentence= lower_case(sentence)\n",
    "    sentence= remove_stop_words(sentence)\n",
    "    sentence= Removing_numbers(sentence)\n",
    "    sentence= Removing_punctuations(sentence)\n",
    "    sentence= Removing_urls(sentence)\n",
    "    sentence= lemmatization(sentence)\n",
    "    return sentence\n",
    "\n",
    "df = pd.read_csv('dataset/emotions.csv')\n",
    "test = df.copy()\n",
    "# drop the tweet_id column\n",
    "df.drop('tweet_id', axis=1, inplace=True)\n",
    "index = df[df['content'].duplicated() == True].index\n",
    "df.drop(index, axis = 0, inplace = True)\n",
    "df.reset_index(inplace=True, drop = True)\n",
    "df = normalize_text(df)\n",
    "\n",
    "# current directory: \n",
    "dir = os.getcwd()\n",
    "path = os.path.join(dir, 'dataset', 'emotions-prepro.csv')\n",
    "df.to_csv(path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('dataset/emotions-prepro.csv')\n",
    "\n",
    "train_test_split = 0.2\n",
    "\n",
    "# shuffle the data\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# split the data into train and test csv files\n",
    "msk = np.random.rand(len(df)) < train_test_split\n",
    "train = df[~msk]\n",
    "test = df[msk]\n",
    "\n",
    "# save the train and test dataframes to csv files\n",
    "train.to_csv('dataset/emotions-train.csv', index=False)\n",
    "test.to_csv('dataset/emotions-test.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 43404\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "df = pd.read_csv('dataset/emotions-prepro.csv')\n",
    "\n",
    "# Convert 'content' column to string type\n",
    "df['content'] = df['content'].astype(str)\n",
    "\n",
    "X = df.content\n",
    "y = df.sentiment\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(y)\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='UNK')\n",
    "test = pd.concat([X_train,X_test])\n",
    "tokenizer.fit_on_texts(test)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "maxlen = max([len(t) for t in df['content']])\n",
    "\n",
    "X_train = pad_sequences(sequences_train, maxlen=maxlen, truncating='pre')\n",
    "X_test = pad_sequences(sequences_test, maxlen=maxlen, truncating='pre')\n",
    "\n",
    "vocabSize = len(tokenizer.index_word) + 1\n",
    "print(f\"Vocabulary size = {vocabSize}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeLabels(y_train, y_test):\n",
    "    \"\"\"\n",
    "    Encode the labels\n",
    "    :param y_train: The training labels\n",
    "    :param y_test: The testing labels\n",
    "    :return: The encoded labels and the labels\n",
    "    \"\"\"\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    labels = le.classes_\n",
    "    print(f\"Labels: {labels} -- {le.transform(labels)}\")\n",
    "    return y_train, y_test, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['anger' 'boredom' 'empty' 'enthusiasm' 'fun' 'happiness' 'hate' 'love'\n",
      " 'neutral' 'relief' 'sadness' 'surprise' 'worry'] -- [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "type <class 'pandas.core.series.Series'>\n",
      "Vocabulary size = 43404\n",
      "Found 400000 word vectors.\n",
      "Converted 20279 words (23124 misses)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_training = pd.read_csv('dataset/emotions-train.csv')\n",
    "df_test = pd.read_csv('dataset/emotions-test.csv')\n",
    "\n",
    "# get the features and labels\n",
    "X_train = df_training.content.astype(str)\n",
    "X_test = df_test.content.astype(str)\n",
    "y_train = df_training.sentiment\n",
    "y_test = df_test.sentiment\n",
    "\n",
    "# encode the labels\n",
    "y_train, y_test, labels = encodeLabels(y_train, y_test)\n",
    "\n",
    "# tokenize the data\n",
    "tokenizer = Tokenizer(oov_token='UNK')\n",
    "print(\"type\",type(X_train))\n",
    "full_text = pd.concat([X_train,X_test])\n",
    "tokenizer.fit_on_texts(pd.concat([X_train,X_test]))\n",
    "\n",
    "# convert text to sequences\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# pad the sequences\n",
    "maxlen = max([len(t) for t in pd.concat([X_train,X_test])])\n",
    "\n",
    "X_train = pad_sequences(sequences_train, maxlen=maxlen, truncating='pre')\n",
    "X_test = pad_sequences(sequences_test, maxlen=maxlen, truncating='pre')\n",
    "\n",
    "vocabSize = len(tokenizer.index_word) + 1\n",
    "print(f\"Vocabulary size = {vocabSize}\")\n",
    "\n",
    "glove_file = os.path.join('dataset', 'glove.6B.200d.txt')\n",
    "num_tokens = vocabSize\n",
    "embedding_dim = 200\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(glove_file, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabSize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tibed\\OneDrive - Hogeschool West-Vlaanderen\\Documenten\\school\\semester 5\\Mlops\\mlops-eindopdracht\\steps.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tibed/OneDrive%20-%20Hogeschool%20West-Vlaanderen/Documenten/school/semester%205/Mlops/mlops-eindopdracht/steps.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tibed/OneDrive%20-%20Hogeschool%20West-Vlaanderen/Documenten/school/semester%205/Mlops/mlops-eindopdracht/steps.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m glove_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mglove.6B.200d.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tibed/OneDrive%20-%20Hogeschool%20West-Vlaanderen/Documenten/school/semester%205/Mlops/mlops-eindopdracht/steps.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m num_tokens \u001b[39m=\u001b[39m vocabSize\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tibed/OneDrive%20-%20Hogeschool%20West-Vlaanderen/Documenten/school/semester%205/Mlops/mlops-eindopdracht/steps.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m embedding_dim \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tibed/OneDrive%20-%20Hogeschool%20West-Vlaanderen/Documenten/school/semester%205/Mlops/mlops-eindopdracht/steps.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m hits \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocabSize' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 136, 200)          8680800   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 136, 256)         336896    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 136, 128)         164352    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 13)                1677      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,282,541\n",
      "Trainable params: 601,741\n",
      "Non-trainable params: 8,680,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional\n",
    "\n",
    "adam = Adam(learning_rate=0.005)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, 200, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.2,recurrent_dropout=0.2)))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 115s 4s/step - loss: 2.1606 - accuracy: 0.2421 - val_loss: 2.0206 - val_accuracy: 0.3044\n"
     ]
    }
   ],
   "source": [
    "# use gpu if available\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "modelCheckpoint = ModelCheckpoint('model/model.h5', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=256, epochs=1, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001), modelCheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 51s 412ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        88\n",
      "           1       0.00      0.00      0.00       150\n",
      "           2       0.00      0.00      0.00       663\n",
      "           3       0.00      0.00      0.00       627\n",
      "           4       0.00      0.00      0.00      1402\n",
      "           5       0.25      0.16      0.20      4192\n",
      "           6       0.00      0.00      0.00      1060\n",
      "           7       0.26      0.37      0.31      2995\n",
      "           8       0.29      0.65      0.40      6862\n",
      "           9       0.00      0.00      0.00      1221\n",
      "          10       0.00      0.00      0.00      4091\n",
      "          11       0.00      0.00      0.00      1739\n",
      "          12       0.33      0.45      0.38      6740\n",
      "\n",
      "    accuracy                           0.29     31830\n",
      "   macro avg       0.09      0.13      0.10     31830\n",
      "weighted avg       0.19      0.29      0.22     31830\n",
      "\n",
      "[[   0    0    0    0    0    3    0    4   46    0    0    0   35]\n",
      " [   0    0    0    0    0    4    0    5   78    0    0    0   63]\n",
      " [   0    0    0    0    0   26    0   30  422    0    0    0  185]\n",
      " [   0    0    0    0    0   55    0   72  338    0    0    0  162]\n",
      " [   0    0    0    0    0  182    0  284  648    0    0    0  288]\n",
      " [   0    0    0    0    0  675    0 1080 1835    0    0    0  602]\n",
      " [   0    0    0    0    0   46    0   36  509    0    0    0  469]\n",
      " [   0    0    0    0    0  500    0 1113 1048    0    0    0  334]\n",
      " [   0    0    0    0    0  414    0  513 4458    0    0    0 1477]\n",
      " [   0    0    0    0    0  107    0  177  593    0    0    0  344]\n",
      " [   0    0    0    0    0  209    0  304 1782    0    0    0 1796]\n",
      " [   0    0    0    0    0  156    0  230  885    0    0    0  468]\n",
      " [   0    0    0    0    0  334    0  426 2967    0    0    0 3013]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tibed\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\tibed\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\tibed\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "classifications = predictions.argmax(axis=1)\n",
    "report = classification_report(y_test.argmax(axis=1), classifications)\n",
    "print(report)\n",
    "cf_matrix = confusion_matrix(y_test.argmax(axis=1), classifications)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(\"model/\"+'test-report.npy'), report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        88\n",
      "           1       0.00      0.00      0.00       150\n",
      "           2       0.00      0.00      0.00       663\n",
      "           3       0.00      0.00      0.00       627\n",
      "           4       0.00      0.00      0.00      1402\n",
      "           5       0.25      0.16      0.20      4192\n",
      "           6       0.00      0.00      0.00      1060\n",
      "           7       0.26      0.37      0.31      2995\n",
      "           8       0.29      0.65      0.40      6862\n",
      "           9       0.00      0.00      0.00      1221\n",
      "          10       0.00      0.00      0.00      4091\n",
      "          11       0.00      0.00      0.00      1739\n",
      "          12       0.33      0.45      0.38      6740\n",
      "\n",
      "    accuracy                           0.29     31830\n",
      "   macro avg       0.09      0.13      0.10     31830\n",
      "weighted avg       0.19      0.29      0.22     31830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load report\n",
    "report = np.load(os.path.join(\"model/\"+'test-report.npy'), allow_pickle='TRUE').item()\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.savefig(os.path.join(\"test\"+'-loss.png'))\n",
    "plt.clf()\n",
    "\n",
    "# plot the accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.savefig(os.path.join(\"test\"+'-accuracy.png'))\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
